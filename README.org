#+TITLE:     Jupyter: Jax
#+AUTHOR:    David Conner
#+DESCRIPTION: notes
#+PROPERTY: header-args:sh     :tangle-mode (identity #o500) :mkdirp yes :shebang #!/bin/bash

Not a poetry project.

* Docs

* Resources

* Build
** Docker

 Find it on [[hub.docker.com/r/rocm/jax-build/tags][hub.docker.com/r/rocm/jax-build/tags]]

#+header: :noweb-ref jax-build-image :noweb-sep ""
#+begin_src emacs-lisp
rocm/jax-build:rocm5.5.0-jax0.4.6.550-py3.8.0
#+end_src

Or use =rocm/tensorflow=

#+header: :noweb-ref tf-rocm-image :noweb-sep ""
#+begin_src emacs-lisp
rocm/tensorflow:latest
#+end_src


Pull the =rocm/jax-build= image. The =:latest= tag doesn't pull.

#+begin_src sh :tangle bin/dpull.jupyter.sh :noweb yes
docker pull <<tf-rocm-image>>
#+end_src

*** Jupyter

Image tag:

#+header: :noweb-ref tf-rocm :noweb-sep ""
#+begin_src emacs-lisp
tf-rocm
#+end_src

Container tag:

#+header: :noweb-ref tf-rocm-jupyter :noweb-sep ""
#+begin_src emacs-lisp
tf-rocm-jupyter
#+end_src

Dockerfile:

#+header: :tangle-mode (identity #o400)
#+begin_src dockerfile :tangle Dockerfile.jupyter :noweb yes
FROM <<tf-rocm-image>>

RUN pip3 install -U pip setuptools
RUN pip3 install jaxlib-rocm
RUN pip3 install jupyterlab pandas~=2.0.0
RUN pip3 install tensorboard
RUN pip3 install geomstats
#+end_src

Don't run this from emacs (no wonder there's no docker-build command)

#+begin_src sh :tangle bin/dbuild.sh :noweb yes
DOCKER_CONTEXT_PATH=$(pwd)
DOCKERFILE_PATH=Dockerfile.jupyter
docker build --tag <<tf-rocm>> -f $DOCKERFILE_PATH $DOCKER_CONTEXT_PATH
#+end_src

**** TODO Run WIth Docker Compose

The compose environment:

#+header: :tangle-mode (identity #o400) :mkdir yes
#+begin_src sh :tangle .env.jupyter :noweb yes
HSA_OVERRIDE_GFX_VERSION=10.3.0
#+end_src

And the docker compose:

#+begin_src yaml :tangle compose-jupyter.yml :noweb yes
version: '3'
services:
  jupyter:
    image: <<tf-rocm>>
    ports:
      - 8080:8888
    environment:
      - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION}
    volumes:
      - ${PWD}/jax:/workspace/jax
      - ${PWD}/nb:/workspace/nb
#+end_src

And the launcher script:

#+begin_src sh :tangle bin/dcomp.sh
# up -d $@ ... hmmmm....
# seems to be crashing unless ran specifically
docker compose --env-file=.env.jupyter -f compose-jupyter.yml
#+end_src

**** Run With Docker

#+begin_src sh :tangle bin/drun.sh :noweb yes
#ROCM_IMAGE=<<tf-rocm-image>>
ROCM_IMAGE=<<tf-rocm>>
docker run \
      -it \
      --network=host \
      --device=/dev/kfd \
      --device=/dev/dri/card0 \
      --device=/dev/dri/renderD128 \
      --ipc=host \
      --shm-size 16G \
      --group-add video \
      --group-add render \
      --cap-add=SYS_PTRACE \
      --security-opt seccomp=unconfined \
      -e JAX_PLATFORMS=cpu,gpu \
      -e HSA_OVERRIDE_GFX_VERSION=10.3.0 \
      -v ${PWD}/jax:/workspace/jax \
      -v ${PWD}/nb:/workspace/nb \
      $ROCM_IMAGE \
      /bin/bash
#+end_src

Dammit, finally.

#+begin_example
root@kratos:/home/jenkins/workspace/jax/releases/jax-release_jaxlib-v0.4.6-rocm55# python
Python 3.8.0 (default, May 10 2023, 04:51:38)
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import tensorflow as tf
2023-05-12 11:10:03.009878: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> tf.config.list_physical_devices()
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
>>>
#+end_example

* External Source

** Diff

#+begin_src diff
diff -u /data/lang/python/nb/jax/src/jaxrocm/ /data/lang/python/nb/jax/src/jax
diff -u /data/lang/python/nb/jax/src/jaxrocm/.bazelrc /data/lang/python/nb/jax/src/jax/.bazelrc
--- /data/lang/python/nb/jax/src/jaxrocm/.bazelrc	2023-05-12 15:32:53.408637469 -0400
+++ /data/lang/python/nb/jax/src/jax/.bazelrc	2023-05-12 15:32:53.322636437 -0400
@@ -1,6 +1,10 @@
 ############################################################################
 # All default build options below.

+# Required by OpenXLA
+# https://github.com/openxla/xla/issues/1323
+build --nocheck_visibility
+
 # Sets the default Apple platform to macOS.
 build --apple_platform_type=macos
 build --macos_minimum_os=10.14
@@ -35,9 +39,9 @@

 # Later Bazel flag values override earlier values; if CUDA/ROCM/TPU are enabled,
 # these values are overridden.
-build --@org_tensorflow//tensorflow/compiler/xla/python:enable_gpu=false
-build --@org_tensorflow//tensorflow/compiler/xla/python:enable_tpu=false
-build --@org_tensorflow//tensorflow/compiler/xla/python:enable_plugin_device=false
+build --@xla//xla/python:enable_gpu=false
+build --@xla//xla/python:enable_tpu=false
+build --@xla//xla/python:enable_plugin_device=false

 ###########################################################################

@@ -65,12 +69,13 @@
 build:cuda --action_env TF_CUDA_COMPUTE_CAPABILITIES="sm_52,sm_60,sm_70,compute_80"
 build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain
 build:cuda --@local_config_cuda//:enable_cuda
-build:cuda --@org_tensorflow//tensorflow/compiler/xla/python:enable_gpu=true
+build:cuda --@xla//xla/python:enable_gpu=true
+build:cuda --@xla//xla/python:jax_cuda_pip_rpaths=true
 build:cuda --define=xla_python_enable_gpu=true

 build:rocm --crosstool_top=@local_config_rocm//crosstool:toolchain
 build:rocm --define=using_rocm=true --define=using_rocm_hipcc=true
-build:rocm --@org_tensorflow//tensorflow/compiler/xla/python:enable_gpu=true
+build:rocm --@xla//xla/python:enable_gpu=true
 build:rocm --define=xla_python_enable_gpu=true
 build:rocm --repo_env TF_NEED_ROCM=1
 build:rocm --action_env TF_ROCM_AMDGPU_TARGETS="gfx900,gfx906,gfx908,gfx90a,gfx1030"
@@ -113,10 +118,10 @@
 # Suppress all warning messages.
 build:short_logs --output_filter=DONT_MATCH_ANYTHING

-build:tpu --@org_tensorflow//tensorflow/compiler/xla/python:enable_tpu=true
+build:tpu --@xla//xla/python:enable_tpu=true
 build:tpu --define=with_tpu_support=true

-build:plugin_device --@org_tensorflow//tensorflow/compiler/xla/python:enable_plugin_device=true
+build:plugin_device --@xla//xla/python:enable_plugin_device=true

 #########################################################################
 # RBE config options below.
@@ -177,55 +182,6 @@
 build:rbe_linux_cuda_base --config=cuda
 build:rbe_linux_cuda_base --repo_env=REMOTE_GPU_TESTING=1

-build:rbe_linux_cuda11.1_nvcc_base --config=rbe_linux_cuda_base
-build:rbe_linux_cuda11.1_nvcc_base --action_env=TF_CUDA_VERSION=11
-build:rbe_linux_cuda11.1_nvcc_base --action_env=TF_CUDNN_VERSION=8
-build:rbe_linux_cuda11.1_nvcc_base --action_env=CUDA_TOOLKIT_PATH="/usr/local/cuda-11.1"
-build:rbe_linux_cuda11.1_nvcc_base --action_env=LD_LIBRARY_PATH="/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/tensorrt/lib"
-build:rbe_linux_cuda11.1_nvcc_base --action_env=GCC_HOST_COMPILER_PATH="/dt9/usr/bin/gcc"
-test:rbe_linux_cuda11.1_nvcc_base --test_env=LD_LIBRARY_PATH="/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-11.1/lib64"
-build:rbe_linux_cuda11.1_nvcc_base --host_crosstool_top="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_cuda//crosstool:toolchain"
-build:rbe_linux_cuda11.1_nvcc_base --crosstool_top="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_cuda//crosstool:toolchain"
-build:rbe_linux_cuda11.1_nvcc_base --extra_toolchains="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_cuda//crosstool:toolchain-linux-x86_64"
-build:rbe_linux_cuda11.1_nvcc_base --extra_execution_platforms="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_platform//:platform"
-build:rbe_linux_cuda11.1_nvcc_base --host_platform="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_platform//:platform"
-build:rbe_linux_cuda11.1_nvcc_base --platforms="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_platform//:platform"
-build:rbe_linux_cuda11.1_nvcc_base --repo_env=TF_CUDA_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_cuda"
-build:rbe_linux_cuda11.1_nvcc_base --repo_env=TF_TENSORRT_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_tensorrt"
-build:rbe_linux_cuda11.1_nvcc_base --repo_env=TF_NCCL_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_nccl"
-build:rbe_linux_cuda11.1_nvcc_py3.8 --config=rbe_linux_cuda11.1_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_python3.8"
-build:rbe_linux_cuda11.1_nvcc_py3.8 --python_path="/usr/local/bin/python3.8"
-build:rbe_linux_cuda11.1_nvcc_py3.9 --config=rbe_linux_cuda11.1_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_python3.9"
-build:rbe_linux_cuda11.1_nvcc_py3.9 --python_path="/usr/local/bin/python3.9"
-build:rbe_linux_cuda11.1_nvcc_py3.10 --config=rbe_linux_cuda11.1_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_python3.10"
-build:rbe_linux_cuda11.1_nvcc_py3.10 --python_path="/usr/local/bin/python3.10"
-build:rbe_linux_cuda11.1_nvcc_py3.11 --config=rbe_linux_cuda11.1_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.1-cudnn8-tensorrt7.2_config_python3.11"
-build:rbe_linux_cuda11.1_nvcc_py3.11 --python_path="/usr/local/bin/python3.11"
-
-build:rbe_linux_cuda11.4_nvcc_base --config=rbe_linux_cuda_base
-build:rbe_linux_cuda11.4_nvcc_base --action_env=TF_CUDA_VERSION=11
-build:rbe_linux_cuda11.4_nvcc_base --action_env=TF_CUDNN_VERSION=8
-build:rbe_linux_cuda11.4_nvcc_base --action_env=CUDA_TOOLKIT_PATH="/usr/local/cuda-11.4"
-build:rbe_linux_cuda11.4_nvcc_base --action_env=LD_LIBRARY_PATH="/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/tensorrt/lib"
-build:rbe_linux_cuda11.4_nvcc_base --action_env=GCC_HOST_COMPILER_PATH="/dt9/usr/bin/gcc"
-build:rbe_linux_cuda11.4_nvcc_base --host_crosstool_top="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_cuda//crosstool:toolchain"
-build:rbe_linux_cuda11.4_nvcc_base --crosstool_top="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_cuda//crosstool:toolchain"
-build:rbe_linux_cuda11.4_nvcc_base --extra_toolchains="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_cuda//crosstool:toolchain-linux-x86_64"
-build:rbe_linux_cuda11.4_nvcc_base --extra_execution_platforms="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_platform//:platform"
-build:rbe_linux_cuda11.4_nvcc_base --host_platform="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_platform//:platform"
-build:rbe_linux_cuda11.4_nvcc_base --platforms="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_platform//:platform"
-build:rbe_linux_cuda11.4_nvcc_base --repo_env=TF_CUDA_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_cuda"
-build:rbe_linux_cuda11.4_nvcc_base --repo_env=TF_TENSORRT_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_tensorrt"
-build:rbe_linux_cuda11.4_nvcc_base --repo_env=TF_NCCL_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_nccl"
-build:rbe_linux_cuda11.4_nvcc_py3.8 --config=rbe_linux_cuda11.4_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_python3.8"
-build:rbe_linux_cuda11.4_nvcc_py3.8 --python_path="/usr/local/bin/python3.8"
-build:rbe_linux_cuda11.4_nvcc_py3.9 --config=rbe_linux_cuda11.4_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_python3.9"
-build:rbe_linux_cuda11.4_nvcc_py3.9 --python_path="/usr/local/bin/python3.9"
-build:rbe_linux_cuda11.4_nvcc_py3.10 --config=rbe_linux_cuda11.4_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_python3.10"
-build:rbe_linux_cuda11.4_nvcc_py3.10 --python_path="/usr/local/bin/python3.10"
-build:rbe_linux_cuda11.4_nvcc_py3.11 --config=rbe_linux_cuda11.4_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.4-cudnn8.2-tensorrt7.2_config_python3.11"
-build:rbe_linux_cuda11.4_nvcc_py3.11 --python_path="/usr/local/bin/python3.11"
-
 build:rbe_linux_cuda11.8_nvcc_base --config=rbe_linux_cuda_base
 build:rbe_linux_cuda11.8_nvcc_base --action_env=TF_CUDA_VERSION=11
 build:rbe_linux_cuda11.8_nvcc_base --action_env=TF_CUDNN_VERSION=8
@@ -250,6 +206,29 @@
 build:rbe_linux_cuda11.8_nvcc_py3.11 --config=rbe_linux_cuda11.8_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda11.8-cudnn8.6-tensorrt8.4_config_python3.11"
 build:rbe_linux_cuda11.8_nvcc_py3.11 --python_path="/usr/local/bin/python3.11"

+build:rbe_linux_cuda12.0.1_nvcc_base --config=rbe_linux_cuda_base
+build:rbe_linux_cuda12.0.1_nvcc_base --action_env=TF_CUDA_VERSION=12
+build:rbe_linux_cuda12.0.1_nvcc_base --action_env=TF_CUDNN_VERSION=8
+build:rbe_linux_cuda12.0.1_nvcc_base --action_env=CUDA_TOOLKIT_PATH="/usr/local/cuda-12"
+build:rbe_linux_cuda12.0.1_nvcc_base --action_env=LD_LIBRARY_PATH="/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/tensorrt/lib"
+build:rbe_linux_cuda12.0.1_nvcc_base --action_env=GCC_HOST_COMPILER_PATH="/dt9/usr/bin/gcc"
+build:rbe_linux_cuda12.0.1_nvcc_base --host_crosstool_top="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_cuda//crosstool:toolchain"
+build:rbe_linux_cuda12.0.1_nvcc_base --crosstool_top="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_cuda//crosstool:toolchain"
+build:rbe_linux_cuda12.0.1_nvcc_base --extra_toolchains="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_cuda//crosstool:toolchain-linux-x86_64"
+build:rbe_linux_cuda12.0.1_nvcc_base --extra_execution_platforms="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_platform//:platform"
+build:rbe_linux_cuda12.0.1_nvcc_base --host_platform="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_platform//:platform"
+build:rbe_linux_cuda12.0.1_nvcc_base --platforms="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_platform//:platform"
+build:rbe_linux_cuda12.0.1_nvcc_base --repo_env=TF_CUDA_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_cuda"
+build:rbe_linux_cuda12.0.1_nvcc_base --repo_env=TF_NCCL_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_nccl"
+build:rbe_linux_cuda12.0.1_nvcc_py3.8 --config=rbe_linux_cuda12.0.1_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_python3.8"
+build:rbe_linux_cuda12.0.1_nvcc_py3.8 --python_path="/usr/local/bin/python3.8"
+build:rbe_linux_cuda12.0.1_nvcc_py3.9 --config=rbe_linux_cuda12.0.1_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_python3.9"
+build:rbe_linux_cuda12.0.1_nvcc_py3.9 --python_path="/usr/local/bin/python3.9"
+build:rbe_linux_cuda12.0.1_nvcc_py3.10 --config=rbe_linux_cuda12.0.1_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_python3.10"
+build:rbe_linux_cuda12.0.1_nvcc_py3.10 --python_path="/usr/local/bin/python3.10"
+build:rbe_linux_cuda12.0.1_nvcc_py3.11 --config=rbe_linux_cuda12.0.1_nvcc_base --repo_env=TF_PYTHON_CONFIG_REPO="@ubuntu20.04-gcc9_manylinux2014-cuda12.0.1-cudnn8.8_config_python3.11"
+build:rbe_linux_cuda12.0.1_nvcc_py3.11 --python_path="/usr/local/bin/python3.11"
+
 # These you may need to change for your own GCP project.
 build:tensorflow_testing_rbe --project_id=tensorflow-testing
 common:tensorflow_testing_rbe_linux --remote_instance_name=projects/tensorflow-testing/instances/default_instance
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/benchmarks and /data/lang/python/nb/jax/src/jax/benchmarks
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/build and /data/lang/python/nb/jax/src/jax/build
diff -u /data/lang/python/nb/jax/src/jaxrocm/CHANGELOG.md /data/lang/python/nb/jax/src/jax/CHANGELOG.md
--- /data/lang/python/nb/jax/src/jaxrocm/CHANGELOG.md	2023-05-12 15:32:53.408637469 -0400
+++ /data/lang/python/nb/jax/src/jax/CHANGELOG.md	2023-05-12 15:32:53.323636449 -0400
@@ -6,7 +6,115 @@
 Remember to align the itemized text with the first line of an item within a list.
 -->

-## jax 0.4.6
+## jax 0.4.11
+
+## jaxlib 0.4.11
+
+## jax 0.4.10 (May 11, 2023)
+
+## jaxlib 0.4.10 (May 11, 2023)
+
+* Changes
+  * Fixed `'apple-m1' is not a recognized processor for this target (ignoring
+    processor)` issue that prevented previous release from running on Mac M1.
+
+## jax 0.4.9 (May 9, 2023)
+
+* Changes
+  * The flags experimental_cpp_jit, experimental_cpp_pjit and
+    experimental_cpp_pmap have been removed.
+    They are now always on.
+  * Accuracy of singular value decomposition (SVD) on TPU has been improved
+    (requires jaxlib 0.4.9).
+
+* Deprecations
+  * `jax.experimental.gda_serialization` is deprecated and has been renamed to
+    `jax.experimental.array_serialization`.
+    Please change your imports to use `jax.experimental.array_serialization`.
+  * The `in_axis_resources` and `out_axis_resources` arguments of pjit have been
+    deprecated. Please use `in_shardings` and `out_shardings` respectively.
+  * The function `jax.numpy.msort` has been removed. It has been deprecated since
+    JAX v0.4.1. Use `jnp.sort(a, axis=0)` instead.
+  * `in_parts` and `out_parts` arguments have been removed from `jax.xla_computation`
+    since they were only used with sharded_jit and sharded_jit is long gone.
+  * `instantiate_const_outputs` argument has been removed from `jax.xla_computation`
+    since it has been unused for a very long time.
+
+## jaxlib 0.4.9 (May 9, 2023)
+
+## jax 0.4.8 (March 29, 2023)
+
+* Breaking changes
+  * A major component of the Cloud TPU runtime has been upgraded. This enables
+    the following new features on Cloud TPU:
+    * {func}`jax.debug.print`, {func}`jax.debug.callback`, and
+      {func}`jax.debug.breakpoint()` now work on Cloud TPU
+    * Automatic TPU memory defragmentation
+
+    {func}`jax.experimental.host_callback` is no longer supported on Cloud TPU
+    with the new runtime component. Please file an issue on the [JAX issue
+    tracker](https://github.com/google/jax/issues) if the new `jax.debug` APIs
+    are insufficient for your use case.
+
+    The old runtime component will be available for at least the next three
+    months by setting the environment variable
+    `JAX_USE_PJRT_C_API_ON_TPU=false`. If you find you need to disable the new
+    runtime for any reason, please let us know on the [JAX issue
+    tracker](https://github.com/google/jax/issues).
+
+* Changes
+  * The minimum jaxlib version has been bumped from 0.4.6 to 0.4.7.
+
+* Deprecations
+  * CUDA 11.4 support has been dropped. JAX GPU wheels only support
+    CUDA 11.8 and CUDA 12. Older CUDA versions may work if jaxlib is built
+    from source.
+  * `global_arg_shapes` argument of pmap only worked with sharded_jit and has
+    been removed from pmap. Please migrate to pjit and remove global_arg_shapes
+    from pmap.
+
+## jax 0.4.7 (March 27, 2023)
+
+* Changes
+  * As per https://jax.readthedocs.io/en/latest/jax_array_migration.html#jax-array-migration
+    `jax.config.jax_array` cannot be disabled anymore.
+  * `jax.config.jax_jit_pjit_api_merge` cannot be disabled anymore.
+  * {func}`jax.experimental.jax2tf.convert` now supports the `native_serialization`
+    parameter to use JAX's native lowering to StableHLO to obtain a
+    StableHLO module for the entire JAX function instead of lowering each JAX
+    primitive to a TensorFlow op. This simplifies the internals and increases
+    the confidence that what you serialize matches the JAX native semantics.
+    See [documentation](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).
+    As part of this change the config flag `--jax2tf_default_experimental_native_lowering`
+    has been renamed to `--jax2tf_native_serialization`.
+  * JAX now depends on `ml_dtypes`, which contains definitions of NumPy types
+    like bfloat16. These definitions were previously internal to JAX, but have
+    been split into a separate package to facilitate sharing them with other
+    projects.
+  * JAX now requires NumPy 1.21 or newer and SciPy 1.7 or newer.
+
+* Deprecations
+  * The type `jax.numpy.DeviceArray` is deprecated. Use `jax.Array` instead,
+    for which it is an alias.
+  * The type `jax.interpreters.pxla.ShardedDeviceArray` is deprecated. Use
+    `jax.Array` instead.
+  * Passing additional arguments to {func}`jax.numpy.ndarray.at` by position is deprecated.
+    For example, instead of `x.at[i].get(True)`, use `x.at[i].get(indices_are_sorted=True)`
+  * `jax.interpreters.xla.device_put` is deprecated. Please use `jax.device_put`.
+  * `jax.interpreters.pxla.device_put` is deprecated. Please use `jax.device_put`.
+  * `jax.experimental.pjit.FROM_GDA` is deprecated. Please pass in sharded
+    jax.Arrays as input and remove the `in_shardings` argument to pjit since
+    it is optional.
+
+## jaxlib 0.4.7 (March 27, 2023)
+
+Changes:
+  * jaxlib now depends on `ml_dtypes`, which contains definitions of NumPy types
+    like bfloat16. These definitions were previously internal to JAX, but have
+    been split into a separate package to facilitate sharing them with other
+    projects.
+
+## jax 0.4.6 (Mar 9, 2023)

 * Changes
   * `jax.tree_util` now contain a set of APIs that allow user to define keys for their
@@ -29,7 +137,7 @@
     * `AttributeKeyPathEntry` : use `GetAttrKey` instead.
     * `GetitemKeyPathEntry` : use `SequenceKey` or `DictKey` instead.

-## jaxlib 0.4.6
+## jaxlib 0.4.6 (Mar 9, 2023)

 ## jax 0.4.5 (Mar 2, 2023)

Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/cloud_tpu_colabs and /data/lang/python/nb/jax/src/jax/cloud_tpu_colabs
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/docs and /data/lang/python/nb/jax/src/jax/docs
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/examples and /data/lang/python/nb/jax/src/jax/examples
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/.git and /data/lang/python/nb/jax/src/jax/.git
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/.github and /data/lang/python/nb/jax/src/jax/.github
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/images and /data/lang/python/nb/jax/src/jax/images
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/jax and /data/lang/python/nb/jax/src/jax/jax
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/jaxlib and /data/lang/python/nb/jax/src/jax/jaxlib
Only in /data/lang/python/nb/jax/src/jaxrocm/: mypy.ini
diff -u /data/lang/python/nb/jax/src/jaxrocm/.pre-commit-config.yaml /data/lang/python/nb/jax/src/jax/.pre-commit-config.yaml
--- /data/lang/python/nb/jax/src/jaxrocm/.pre-commit-config.yaml	2023-05-12 15:32:53.408637469 -0400
+++ /data/lang/python/nb/jax/src/jax/.pre-commit-config.yaml	2023-05-12 15:32:53.322636437 -0400
@@ -8,6 +8,19 @@
 # 'pre-commit run --all'

 repos:
+- repo: https://github.com/pre-commit/pre-commit-hooks
+  rev: v4.3.0
+  hooks:
+  - id: end-of-file-fixer
+    # only include python files
+    files: \.py$
+  - id: debug-statements
+    # only include python files
+    files: \.py$
+  - id: trailing-whitespace
+    # only include python files
+    files: \.py$
+
 - repo: https://github.com/pycqa/flake8
   rev: '6.0.0'
   hooks:
@@ -19,7 +32,8 @@
   - id: mypy
     files: (jax/|tests/typing_test\.py)
     exclude: jax/_src/basearray.py  # Use pyi instead
-    additional_dependencies: [types-requests==2.28.11, jaxlib==0.4.1]
+    additional_dependencies: [types-requests==2.29.0, jaxlib==0.4.7, ml_dtypes==0.1.0, numpy==1.24.3, scipy==1.10.1]
+    args: [--config=pyproject.toml]

 - repo: https://github.com/mwouts/jupytext
   rev: v1.14.4
Only in /data/lang/python/nb/jax/src/jaxrocm/: pylintrc
Only in /data/lang/python/nb/jax/src/jax: pyproject.toml
Only in /data/lang/python/nb/jax/src/jaxrocm/: pytest.ini
diff -u /data/lang/python/nb/jax/src/jaxrocm/README.md /data/lang/python/nb/jax/src/jax/README.md
--- /data/lang/python/nb/jax/src/jaxrocm/README.md	2023-05-12 15:32:53.409637481 -0400
+++ /data/lang/python/nb/jax/src/jax/README.md	2023-05-12 15:32:53.323636449 -0400
@@ -143,7 +143,7 @@
 forward-mode Jacobian-vector products. The two can be composed arbitrarily with
 one another, and with other JAX transformations. Here's one way to compose those
 to make a function that efficiently computes [full Hessian
-matrices](https://jax.readthedocs.io/en/latest/jax.html#jax.hessian):
+matrices](https://jax.readthedocs.io/en/latest/_autosummary/jax.hessian.html#jax.hessian):

 ```python
 from jax import jit, jacfwd, jacrev
@@ -417,67 +417,83 @@
 **These `pip` installations do not work with Windows, and may fail silently; see
 [above](#installation).**

-### pip installation: GPU (CUDA)
+### pip installation: GPU (CUDA, installed via pip, easier)

-If you want to install JAX with both CPU and NVidia GPU support, you must first
-install [CUDA](https://developer.nvidia.com/cuda-downloads) and
-[CuDNN](https://developer.nvidia.com/CUDNN),
-if they have not already been installed. Unlike some other popular deep
-learning systems, JAX does not bundle CUDA or CuDNN as part of the `pip`
-package.
-
-JAX provides pre-built CUDA-compatible wheels for **Linux only**,
-with CUDA 11.4 or newer, and CuDNN 8.2 or newer. Note these existing wheels are currently for `x86_64` architectures only. Other combinations of
-operating system, CUDA, and CuDNN are possible, but require [building from
-source](https://jax.readthedocs.io/en/latest/developer.html#building-from-source).
-
-* CUDA 11.4 or newer is *required*.
-  * Your CUDA installation must be new enough to support your GPU. If you have
-    an Ada Lovelace (e.g., RTX 4080) or Hopper (e.g., H100) GPU,
-    you must use CUDA 11.8 or newer.
-* The supported cuDNN versions for the prebuilt wheels are:
-  * cuDNN 8.6 or newer. We recommend using the cuDNN 8.6 wheel if your cuDNN
-    installation is new enough, since it supports additional functionality.
-  * cuDNN 8.2 or newer.
-* You *must* use an NVidia driver version that is at least as new as your
-  [CUDA toolkit's corresponding driver version](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-major-component-versions__table-cuda-toolkit-driver-versions).
-  For example, if you have CUDA 11.4 update 4 installed, you must use NVidia
-  driver 470.82.01 or newer if on Linux. This is a strict requirement that
-  exists because JAX relies on JIT-compiling code; older drivers may lead to
-  failures.
-  * If you need to use an newer CUDA toolkit with an older driver, for example
-    on a cluster where you cannot update the NVidia driver easily, you may be
-    able to use the
-    [CUDA forward compatibility packages](https://docs.nvidia.com/deploy/cuda-compatibility/)
-    that NVidia provides for this purpose.
+There are two ways to install JAX with NVIDIA GPU support: using CUDA and CUDNN
+installed from pip wheels, and using a self-installed CUDA/CUDNN. We recommend
+installing CUDA and CUDNN using the pip wheels, since it is much easier!
+
+You must first install the NVIDIA driver. We
+recommend installing the newest driver available from NVIDIA, but the driver
+must be version >= 525.60.13 for CUDA 12 and >= 450.80.02 for CUDA 11 on Linux.
+If you need to use an newer CUDA toolkit with an older driver, for example
+on a cluster where you cannot update the NVIDIA driver easily, you may be
+able to use the
+[CUDA forward compatibility packages](https://docs.nvidia.com/deploy/cuda-compatibility/)
+that NVIDIA provides for this purpose.


-Next, run
-
 ```bash
 pip install --upgrade pip
-# Installs the wheel compatible with CUDA 11 and cuDNN 8.6 or newer.
+
+# CUDA 12 installation
 # Note: wheels only available on linux.
-pip install --upgrade "jax[cuda]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
+pip install --upgrade "jax[cuda12_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
+
+# CUDA 11 installation
+# Note: wheels only available on linux.
+pip install --upgrade "jax[cuda11_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
 ```

-**These `pip` installations do not work with Windows, and may fail silently; see
-[above](#installation).**
+### pip installation: GPU (CUDA, installed locally, harder)
+
+If you prefer to use a preinstalled copy of CUDA, you must first
+install [CUDA](https://developer.nvidia.com/cuda-downloads) and
+[CuDNN](https://developer.nvidia.com/CUDNN).
+
+JAX provides pre-built CUDA-compatible wheels for **Linux x86_64 only**. Other
+combinations of operating system and architecture are possible, but require
+[building from source](https://jax.readthedocs.io/en/latest/developer.html#building-from-source).
+
+You should use an NVIDIA driver version that is at least as new as your
+[CUDA toolkit's corresponding driver version](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-major-component-versions__table-cuda-toolkit-driver-versions).
+If you need to use an newer CUDA toolkit with an older driver, for example
+on a cluster where you cannot update the NVIDIA driver easily, you may be
+able to use the
+[CUDA forward compatibility packages](https://docs.nvidia.com/deploy/cuda-compatibility/)
+that NVIDIA provides for this purpose.
+
+JAX currently ships two CUDA wheel variants:
+* CUDA 12.0 and CuDNN 8.8.
+* CUDA 11.8 and CuDNN 8.6.
+
+You may use a JAX wheel provided the major version of your CUDA and CuDNN
+installation matches, and the minor version is at least as new as the version
+JAX expects. For example, you would be able to use the CUDA 12.0 wheel with
+CUDA 12.1 and CuDNN 8.9.
+
+Your CUDA installation must also be new enough to support your GPU. If you have
+an Ada Lovelace (e.g., RTX 4080) or Hopper (e.g., H100) GPU,
+you must use CUDA 11.8 or newer.
+

-The jaxlib version must correspond to the version of the existing CUDA
-installation you want to use. You can specify a particular CUDA and CuDNN
-version for jaxlib explicitly:
+To install, run

 ```bash
 pip install --upgrade pip

-# Installs the wheel compatible with Cuda >= 11.8 and cudnn >= 8.6
-pip install "jax[cuda11_cudnn86]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
+# Installs the wheel compatible with CUDA 12 and cuDNN 8.8 or newer.
+# Note: wheels only available on linux.
+pip install --upgrade "jax[cuda12_local]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

-# Installs the wheel compatible with Cuda >= 11.4 and cudnn >= 8.2
-pip install "jax[cuda11_cudnn82]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
+# Installs the wheel compatible with CUDA 11 and cuDNN 8.6 or newer.
+# Note: wheels only available on linux.
+pip install --upgrade "jax[cuda11_local]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
 ```

+**These `pip` installations do not work with Windows, and may fail silently; see
+[above](#installation).**
+
 You can find your CUDA version with the command:

 ```bash
@@ -506,13 +522,17 @@
 ```

 ### pip installation: Colab TPU
-Colab TPU runtimes come with JAX pre-installed, but before importing JAX you must run the following code to initialize the TPU:
+Colab TPU runtimes use an older TPU architecture than Cloud TPU VMs, so the installation instructions differ.
+The Colab TPU runtime comes with JAX pre-installed, but before importing JAX you must run the following code to initialize the TPU:
 ```python
 import jax.tools.colab_tpu
 jax.tools.colab_tpu.setup_tpu()
 ```
-Colab TPU runtimes use an older TPU architecture than Cloud TPU VMs, so installing `jax[tpu]` should be avoided on Colab.
-If for any reason you would like to update the jax & jaxlib libraries on a Colab TPU runtime, follow the CPU instructions above (i.e. install `jax[cpu]`).
+Note that Colab TPU runtimes are not compatible with JAX version 0.4.0 or newer.
+If you need to re-install JAX on a Colab TPU runtime, you can use the following command:
+```
+!pip install jax<=0.3.25 jaxlib<=0.3.25
+```

 ### Conda installation

@@ -523,7 +543,7 @@
 conda install jax -c conda-forge
 ```

-To install on a machine with an NVidia GPU, run
+To install on a machine with an NVIDIA GPU, run
 ```bash
 conda install jaxlib=*=*cuda* jax cuda-nvcc -c conda-forge -c nvidia
 ```
diff -u /data/lang/python/nb/jax/src/jaxrocm/setup.cfg /data/lang/python/nb/jax/src/jax/setup.cfg
--- /data/lang/python/nb/jax/src/jaxrocm/setup.cfg	2023-05-12 15:32:53.480638333 -0400
+++ /data/lang/python/nb/jax/src/jax/setup.cfg	2023-05-12 15:32:53.391637265 -0400
@@ -38,6 +38,7 @@
   jax/interpreters/ad.py:F401
   jax/interpreters/batching.py:F401
   jax/interpreters/mlir.py:F401
+  jax/interpreters/partial_eval.py:F401
   jax/interpreters/pxla.py:F401
   jax/interpreters/xla.py:F401
   jax/linear_util.py:F401
@@ -53,6 +54,7 @@
   jax/util.py:F401
   jax/_src/api.py:F401
   jax/_src/numpy/lax_numpy.py:F401
+  jax/_src/typing.py:F401
   jax/experimental/*.py:F401
   jax/lax/*.py:F401
   jax/nn/*.py:F401
diff -u /data/lang/python/nb/jax/src/jaxrocm/setup.py /data/lang/python/nb/jax/src/jax/setup.py
--- /data/lang/python/nb/jax/src/jaxrocm/setup.py	2023-05-12 15:32:53.480638333 -0400
+++ /data/lang/python/nb/jax/src/jax/setup.py	2023-05-12 15:32:53.391637265 -0400
@@ -19,14 +19,13 @@

 from setuptools import setup, find_packages

-_current_jaxlib_version = '0.4.6'
+_current_jaxlib_version = '0.4.10'
 # The following should be updated with each new jaxlib release.
-_latest_jaxlib_version_on_pypi = '0.4.4'
-_available_cuda_versions = ['11']
-_default_cuda_version = '11'
-_available_cudnn_versions = ['82', '86']
-_default_cudnn_version = '86'
-_libtpu_version = '0.1.dev20230309'
+_latest_jaxlib_version_on_pypi = '0.4.10'
+_available_cuda11_cudnn_versions = ['82', '86']
+_default_cuda11_cudnn_version = '86'
+_default_cuda12_cudnn_version = '88'
+_libtpu_version = '0.1.dev20230511'

 _dct = {}
 with open('jax/version.py', encoding='utf-8') as f:
@@ -64,9 +63,10 @@
     package_data={'jax': ['py.typed', "*.pyi", "**/*.pyi"]},
     python_requires='>=3.8',
     install_requires=[
-        'numpy>=1.20',
+        'ml_dtypes>=0.1.0',
+        'numpy>=1.21',
         'opt_einsum',
-        'scipy>=1.5',
+        'scipy>=1.7',
     ],
     extras_require={
         # Minimum jaxlib version; used in testing.
@@ -89,16 +89,49 @@
         # $ pip install jax[australis]
         'australis': ['protobuf>=3.13,<4'],

-        # CUDA installations require adding jax releases URL; e.g.
+        # CUDA installations require adding the JAX CUDA releases URL, e.g.,
         # Cuda installation defaulting to a CUDA and Cudnn version defined above.
         # $ pip install jax[cuda] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
-        'cuda': [f"jaxlib=={_current_jaxlib_version}+cuda{_default_cuda_version}.cudnn{_default_cudnn_version}"],
+        'cuda': [f"jaxlib=={_current_jaxlib_version}+cuda11.cudnn{_default_cuda11_cudnn_version}"],
+
+        'cuda11_pip': [
+          f"jaxlib=={_current_jaxlib_version}+cuda11.cudnn{_default_cuda11_cudnn_version}",
+          "nvidia-cublas-cu11>=11.11",
+          "nvidia-cuda-cupti-cu11>=11.8",
+          "nvidia-cuda-nvcc-cu11>=11.8",
+          "nvidia-cuda-runtime-cu11>=11.8",
+          "nvidia-cudnn-cu11>=8.6",
+          "nvidia-cufft-cu11>=10.9",
+          "nvidia-cusolver-cu11>=11.4",
+          "nvidia-cusparse-cu11>=11.7",
+        ],
+
+        'cuda12_pip': [
+          f"jaxlib=={_current_jaxlib_version}+cuda12.cudnn{_default_cuda12_cudnn_version}",
+          "nvidia-cublas-cu12",
+          "nvidia-cuda-cupti-cu12",
+          "nvidia-cuda-nvcc-cu12",
+          "nvidia-cuda-runtime-cu12",
+          "nvidia-cudnn-cu12",
+          "nvidia-cufft-cu12",
+          "nvidia-cusolver-cu12",
+          "nvidia-cusparse-cu12",
+        ],
+
+        # Target that does not depend on the CUDA pip wheels, for those who want
+        # to use a preinstalled CUDA.
+        'cuda11_local': [
+          f"jaxlib=={_current_jaxlib_version}+cuda11.cudnn{_default_cuda11_cudnn_version}",
+        ],
+        'cuda12_local': [
+          f"jaxlib=={_current_jaxlib_version}+cuda12.cudnn{_default_cuda12_cudnn_version}",
+        ],

         # CUDA installations require adding jax releases URL; e.g.
         # $ pip install jax[cuda11_cudnn82] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
         # $ pip install jax[cuda11_cudnn86] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
-        **{f'cuda{cuda_version}_cudnn{cudnn_version}': f"jaxlib=={_current_jaxlib_version}+cuda{cuda_version}.cudnn{cudnn_version}"
-           for cuda_version in _available_cuda_versions for cudnn_version in _available_cudnn_versions}
+        **{f'cuda11_cudnn{cudnn_version}': f"jaxlib=={_current_jaxlib_version}+cuda11.cudnn{cudnn_version}"
+           for cudnn_version in _available_cuda11_cudnn_versions}
     },
     url='https://github.com/google/jax',
     license='Apache-2.0',
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/tests and /data/lang/python/nb/jax/src/jax/tests
Common subdirectories: /data/lang/python/nb/jax/src/jaxrocm/third_party and /data/lang/python/nb/jax/src/jax/third_party
diff -u /data/lang/python/nb/jax/src/jaxrocm/WORKSPACE /data/lang/python/nb/jax/src/jax/WORKSPACE
--- /data/lang/python/nb/jax/src/jaxrocm/WORKSPACE	2023-05-12 15:32:53.409637481 -0400
+++ /data/lang/python/nb/jax/src/jax/WORKSPACE	2023-05-12 15:32:53.323636449 -0400
@@ -1,16 +1,16 @@
 load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive")

-# To update TensorFlow to a new revision,
+# To update XLA to a new revision,
 # a) update URL and strip_prefix to the new git commit hash
 # b) get the sha256 hash of the commit by running:
-#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
+#    curl -L https://github.com/openxla/xla/archive/<git hash>.tar.gz | sha256sum
 #    and update the sha256 with the result.
 http_archive(
-    name = "org_tensorflow",
-    sha256 = "08fd0ab0b672510229ad2fff276a3634f205fc539fa16a5bdeeaaccd881ece27",
-    strip_prefix = "tensorflow-2aaeef25361311b21b9e81e992edff94bcb6bae3",
+    name = "xla",
+    sha256 = "14bbdd796211e45a7a3148bd5e43523da4556ff19cf5461588b79650b55b9302",
+    strip_prefix = "xla-c1e4a16e77a7ba2000003ccade3ffba3749ada35",
     urls = [
-        "https://github.com/tensorflow/tensorflow/archive/2aaeef25361311b21b9e81e992edff94bcb6bae3.tar.gz",
+        "https://github.com/openxla/xla/archive/c1e4a16e77a7ba2000003ccade3ffba3749ada35.tar.gz",
     ],
 )

@@ -19,26 +19,32 @@
 # local checkout by either:
 # a) overriding the TF repository on the build.py command line by passing a flag
 #    like:
-#    python build/build.py --bazel_options=--override_repository=org_tensorflow=/path/to/tensorflow
+#    python build/build.py --bazel_options=--override_repository=xla=/path/to/xla
 #    or
 # b) by commenting out the http_archive above and uncommenting the following:
 # local_repository(
-#    name = "org_tensorflow",
-#    path = "/path/to/tensorflow",
+#    name = "xla",
+#    path = "/path/to/xla",
 # )

 load("//third_party/ducc:workspace.bzl", ducc = "repo")
 ducc()

-# Initialize TensorFlow's external dependencies.
-load("@org_tensorflow//tensorflow:workspace3.bzl", "tf_workspace3")
-tf_workspace3()
+load("@xla//:workspace4.bzl", "xla_workspace4")
+xla_workspace4()

-load("@org_tensorflow//tensorflow:workspace2.bzl", "tf_workspace2")
-tf_workspace2()
+load("@xla//:workspace3.bzl", "xla_workspace3")
+xla_workspace3()

-load("@org_tensorflow//tensorflow:workspace1.bzl", "tf_workspace1")
-tf_workspace1()
+load("@xla//:workspace2.bzl", "xla_workspace2")
+xla_workspace2()

-load("@org_tensorflow//tensorflow:workspace0.bzl", "tf_workspace0")
-tf_workspace0()
+load("@xla//:workspace1.bzl", "xla_workspace1")
+xla_workspace1()
+
+load("@xla//:workspace0.bzl", "xla_workspace0")
+xla_workspace0()
+
+
+load("//third_party/flatbuffers:workspace.bzl", flatbuffers = "repo")
+flatbuffers()

Diff finished.  Fri May 12 15:36:25 2023
#+end_src

** Repo

#+begin_src xml :tangle default.xml
<?xml version="1.0" encoding="utf-8"?>
<manifest>
  <remote name="github" fetch="https://github.com"/>
  <default remote="github" sync-j="8" revision="refs/heads/master"/>
  <project name="google/jax" path="jax" revision="refs/heads/main" remote="github" />
  <project name="google/jax" path="jax046" revision="refs/tags/jaxlib-v0.4.6" remote="github" />
  <project name="ROCmSoftwarePlatform/jax" path="jaxrocm" revision="refs/tags/jaxlib-v0.4.6-rocm55" remote="github" />
  <project name="geomstats/geomstats" path="geomstats" revision="refs/heads/master" remote="github" />
</manifest>
#+end_src

Can sync with:

#+begin_src sh
export WD=$(pwd)
cd src
#repo init file://$WD/default.xml --standalone-manifest
repo init -u file://$WD -m default.xml

if [ $? -eq 0 ]; then
    repo sync
else
    echo "couldn't repo init" && exit 1
fi
#+end_src

I haven't tested =repo sync= from a single file, but the above script almost
does it. It gets =default.xml= into =src/.repo/manifests=.

*** Just testing this repo workflow

This method can't be used for CI, all the remotes need to be specified in the
file and some features don't work. Still, I just want the code for reference --
I think? Why? I found [[https://github.com/google/jax/blob/main/docs/autodidax.md][autodidax.md]] _because it was local._

I'm just experimenting with different workflows here, since =git submodules= for
each thing i'm interested in is exhausting. Other options are:

+ Branches, but I have to switch each time
+ Files, but I think subject to the same limitation
+ Orphan branches but ... blech even more confusing.
+ XML Manifest Server: probably the better option.

* Issues

** The =rocm/jax-build= image lacks python with sqlite3

From here I can't rebuild python or install a new python. AFAIK the JAX branch
isn't on the system.

#+begin_example
root@kratos:/# python
Python 3.8.0 (default, May 10 2023, 04:51:38)
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import sqlite3
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "/pyenv/versions/3.8.0/lib/python3.8/sqlite3/__init__.py", line 23, in <module>
from sqlite3.dbapi2 import *
File "/pyenv/versions/3.8.0/lib/python3.8/sqlite3/dbapi2.py", line 27, in <module>
from _sqlite3 import *
ModuleNotFoundError: No module named '_sqlite3'
#+end_example

According to [[github:google/jax/issues/15983][google/jax#15983]], I can simply install the jaxlib.whl on top of the
=rcom/tensorflow= image

** Getting JAX to recognize the ROCm GPU device

The =rocm/tensorflow= image with =jaxlib-rocm= installed doesn't initially
recognize my ROCm device even though it shows in =tf.config.list_physical_devices()=.

#+begin_src python
import jax.numpy as jnp
from jax import config as jcfg
from jax import grad, jit, vmap
from jax import random

key = random.PRNGKey(0)

#jcfg.jax_platforms
#+end_src

The above will either: warn: =No GPU/TPU= or error with:

#+begin_example
Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (set JAX_PLATFORMS='' to automatically choose an available backend)
#+end_example

Tracing through the following should help. The JAX platform parameters are
initialized with flags & environment variables in  =jax/_src/config.py=.

#+begin_src python
from jax._src.lib import xla_client
from jax._src.lib import xla_bridge
from jax._src.config import flags

#FLAGS = flags.FLAGS
#FLAGS.jax_platflorm_name

xla_bridge.is_known_platform('rocm') # True
xla_client.CompileOptions()

xla_bridge.backends()
#+end_src

* Notebooks
